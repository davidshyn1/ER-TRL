{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d9ee211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "/home/skshyn/miniconda3/envs/omrl/lib/python3.12/site-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
      "  from distutils.dep_util import newer, newer_group\n",
      "<frozen importlib._bootstrap>:530: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import module: half_cheetah_vel\n",
      "import module: humanoid_dir\n",
      "import module: ant_dir\n",
      "import module: hopper_params\n",
      "import module: ant_goal\n",
      "import module: walker_params\n",
      "import module: point_robot\n",
      "import module: half_cheetah_dir\n",
      "import module: wrappers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: DeprecationWarning: Type google.protobuf.pyext._message.ScalarMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.\n",
      "<frozen importlib._bootstrap>:488: DeprecationWarning: Type google.protobuf.pyext._message.MessageMapContainer uses PyType_Spec with a metaclass that has custom tp_new. This is deprecated and will no longer be allowed in Python 3.14.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "from launch_experiment import initialize as init_agent\n",
    "from hydra import initialize, compose\n",
    "import rlkit.torch.pytorch_util as ptu\n",
    "from scripts.mine.mine import get_estimator\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ptu.set_gpu_mode(torch.cuda.is_available())\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf3096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_agent(env, algo='offlinepearl'):\n",
    "    \"\"\"Load trained agent and config\"\"\"\n",
    "    with initialize(version_base=\"1.3\", config_path=\"../cfgs\"):\n",
    "        cfg = compose('experiment', overrides=[f'+env={env}', f'+algo={algo}'])\n",
    "    agent = init_agent(cfg)\n",
    "    return agent, cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cb551a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Networks initialized -------------\n",
      "[Network] Total number of parameters : 0.235 M\n",
      "-----------------------------------------------\n",
      "---------- Networks initialized -------------\n",
      "[Network] Total number of parameters : 0.144 M\n",
      "-----------------------------------------------\n",
      "---------- Networks initialized -------------\n",
      "[Network] Total number of parameters : 0.144 M\n",
      "-----------------------------------------------\n",
      "---------- Networks initialized -------------\n",
      "[Network] Total number of parameters : 0.142 M\n",
      "-----------------------------------------------\n",
      "---------- Networks initialized -------------\n",
      "[Network] Total number of parameters : 0.144 M\n",
      "-----------------------------------------------\n",
      "---------- Networks initialized -------------\n",
      "[Network] Total number of parameters : 0.094 M\n",
      "-----------------------------------------------\n",
      "Environment: cheetah-vel\n",
      "Training tasks: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "Eval tasks: [5, 6, 7, 8, 9, 30, 31, 32, 33, 34]\n",
      "\n",
      "Dimensions:\n",
      "  State (observation): 20\n",
      "  Action: 6\n",
      "  Latent: 20\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "# Load agent\n",
    "env_name = 'cheetah-vel'\n",
    "agent, cfg = load_agent(env=env_name, algo='offlinepearl')\n",
    "\n",
    "print(f\"Environment: {env_name}\")\n",
    "print(f\"Training tasks: {agent.train_tasks}\")\n",
    "print(f\"Eval tasks: {agent.eval_tasks}\")\n",
    "\n",
    "# Get dimensions from agent's environment\n",
    "state_dim = int(np.prod(agent.env.observation_space.shape))  # observation_dim\n",
    "action_dim = int(np.prod(agent.env.action_space.shape))\n",
    "reward_dim = 1\n",
    "latent_dim = agent.latent_dim  # This is stored in the algorithm object\n",
    "\n",
    "print(f\"\\nDimensions:\")\n",
    "print(f\"  State (observation): {state_dim}\")\n",
    "print(f\"  Action: {action_dim}\")\n",
    "print(f\"  Latent: {latent_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aa76f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_with_latent(agent, task_idx, include_action=True, batch_size=64, is_training=None):\n",
    "    \"\"\"\n",
    "    Get batch from agent's buffer and compute latent representation Z\n",
    "    \n",
    "    Args:\n",
    "        agent: The loaded agent\n",
    "        task_idx: Task index (single integer or list of task indices)\n",
    "        include_action: Whether to include action in the context\n",
    "        batch_size: Batch size\n",
    "        is_training: If True, use replay_buffer; if False, use eval_buffer; \n",
    "                     if None, automatically determine based on task_idx\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (joint, marginal) tensors\n",
    "    \"\"\"\n",
    "    # Handle both single task index and list of task indices\n",
    "    if not hasattr(task_idx, '__iter__') or isinstance(task_idx, (str, bytes)):\n",
    "        task_indices = [task_idx]\n",
    "    else:\n",
    "        task_indices = list(task_idx)\n",
    "    \n",
    "    # Auto-determine buffer if is_training is None\n",
    "    if is_training is None:\n",
    "        # Check if task is in train_tasks or eval_tasks\n",
    "        if task_indices[0] in agent.train_tasks:\n",
    "            is_training = True\n",
    "        elif task_indices[0] in agent.eval_tasks:\n",
    "            is_training = False\n",
    "        else:\n",
    "            # Default to training buffer if not found in either\n",
    "            is_training = True\n",
    "            print(f\"Warning: task {task_indices[0]} not found in train_tasks or eval_tasks, using replay_buffer\")\n",
    "\n",
    "    # Get batch from appropriate buffer\n",
    "    if is_training:\n",
    "        batches = [ptu.np_to_pytorch_batch(agent.replay_buffer.random_batch(idx, batch_size=batch_size)) \n",
    "                   for idx in task_indices]\n",
    "        marginal_batches1 = [ptu.np_to_pytorch_batch(agent.replay_buffer.random_batch(idx, batch_size=batch_size)) \n",
    "                   for idx in task_indices]\n",
    "        marginal_batches2 = [ptu.np_to_pytorch_batch(agent.replay_buffer.random_batch(idx, batch_size=batch_size)) \n",
    "                   for idx in task_indices]\n",
    "    else:\n",
    "        batches = [ptu.np_to_pytorch_batch(agent.eval_buffer.random_batch(idx, batch_size=batch_size)) \n",
    "                   for idx in task_indices]\n",
    "        marginal_batches1 = [ptu.np_to_pytorch_batch(agent.eval_buffer.random_batch(idx, batch_size=batch_size)) \n",
    "                   for idx in task_indices]\n",
    "        marginal_batches2 = [ptu.np_to_pytorch_batch(agent.eval_buffer.random_batch(idx, batch_size=batch_size)) \n",
    "                   for idx in task_indices]\n",
    "    \n",
    "    # Use agent's unpack_batch method (same as sample_context in agent)\n",
    "    unpacked = [agent.unpack_batch(batch, sparse_reward=False) for batch in batches]\n",
    "    marginal_unpacked1 = [agent.unpack_batch(batch, sparse_reward=False) for batch in marginal_batches1]\n",
    "    marginal_unpacked2 = [agent.unpack_batch(batch, sparse_reward=False) for batch in marginal_batches2]\n",
    "    # unpacked is [[o1, a1, r1, no1, t1], [o2, a2, r2, no2, t2], ...]\n",
    "    \n",
    "    # Group like elements together: [[o1, o2, ...], [a1, a2, ...], [r1, r2, ...], ...]\n",
    "    unpacked = [[x[i] for x in unpacked] for i in range(len(unpacked[0]))]\n",
    "    marginal_unpacked1 = [[x[i] for x in marginal_unpacked1] for i in range(len(marginal_unpacked1[0]))]\n",
    "    marginal_unpacked2 = [[x[i] for x in marginal_unpacked2] for i in range(len(marginal_unpacked2[0]))]\n",
    "\n",
    "\n",
    "    # Concatenate each group: [all_o, all_a, all_r, all_no, all_t]\n",
    "    unpacked = [torch.cat(x, dim=0) for x in unpacked]\n",
    "    marginal_unpacked1 = [torch.cat(x, dim=0) for x in marginal_unpacked1]\n",
    "    marginal_unpacked2 = [torch.cat(x, dim=0) for x in marginal_unpacked2]\n",
    "\n",
    "    # Get context encoder\n",
    "    context_encoder = agent.agent.context_encoder\n",
    "    context_encoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Create context using agent's existing method\n",
    "        if agent.use_next_obs_in_context:\n",
    "            context_input = torch.cat(unpacked[:-1], dim=2)  # [obs, act, rewards, next_obs]\n",
    "            marginal_context_input1 = torch.cat(marginal_unpacked1[:-1], dim=2)\n",
    "            marginal_context_input2 = torch.cat(marginal_unpacked2[:-1], dim=2)\n",
    "        else:\n",
    "            context_input = torch.cat(unpacked[:-2], dim=2)  # [obs, act, rewards]\n",
    "            marginal_context_input1 = torch.cat(marginal_unpacked1[:-2], dim=2)\n",
    "            marginal_context_input2 = torch.cat(marginal_unpacked2[:-2], dim=2)\n",
    "        \n",
    "        context_input = context_input.reshape(-1, context_input.shape[2])\n",
    "        marginal_context_input1 = marginal_context_input1.reshape(-1, marginal_context_input1.shape[2])\n",
    "        marginal_context_input2 = marginal_context_input2.reshape(-1, marginal_context_input2.shape[2])\n",
    "        # Encode to get latent representation Z\n",
    "        z_raw = context_encoder(context_input)\n",
    "        marginal_z_raw2 = context_encoder(marginal_context_input2)\n",
    "        \n",
    "        # Handle information bottleneck case (latent_dim * 2 -> mean, std)\n",
    "        if z_raw.shape[1] == agent.agent.latent_dim * 2:\n",
    "            z = z_raw[:, :agent.agent.latent_dim]\n",
    "            marginal_z = marginal_z_raw2[:, :agent.agent.latent_dim]\n",
    "        else:\n",
    "            z = z_raw\n",
    "            marginal_z = marginal_z_raw2\n",
    "        if include_action:\n",
    "            pass\n",
    "        else:\n",
    "            state_dim = int(np.prod(agent.env.observation_space.shape))  # observation_dim\n",
    "            action_dim = int(np.prod(agent.env.action_space.shape))\n",
    "            context_input = torch.cat([context_input[:, :state_dim], context_input[:, state_dim + action_dim:]], dim=1)\n",
    "            marginal_context_input1 = torch.cat([marginal_context_input1[:, :state_dim], marginal_context_input1[:, state_dim + action_dim:]], dim=1)\n",
    "        return torch.cat([z, context_input], dim=1), torch.cat([marginal_z, marginal_context_input1], dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bd392b",
   "metadata": {},
   "source": [
    "### MINE with IB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2ae75c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mine_model(agent, train_tasks, latent_dim, action_dim, state_dim, reward_dim,\n",
    "                    epochs=100, batch_size=256, samples_per_task=1000, lr=2e-4, estimator='dv', device='cpu'):\n",
    "    \"\"\"\n",
    "    Train MINE model to estimate I(Z; A | S, R, S')\n",
    "    \n",
    "    For conditional MI I(Z; A | S, R, S'), we use the chain rule:\n",
    "    I(Z; A | S, R, S') = I(Z; A, S, R, S') - I(Z; S, R, S')\n",
    "    \n",
    "    We estimate this by training two MINE models:\n",
    "    1. I(Z; A, S, R, S') = I(Z; concat(A, S, R, S'))\n",
    "    2. I(Z; S, R, S') = I(Z; concat(S, R, S'))\n",
    "    \n",
    "    Then: I(Z; A | S, R, S') = I(Z; A, S, R, S') - I(Z; S, R, S')\n",
    "    \n",
    "    Args:\n",
    "        agent: The loaded agent\n",
    "        train_tasks: List of training task indices\n",
    "        latent_dim: Dimension of latent Z\n",
    "        action_dim: Dimension of action A\n",
    "        state_dim: Dimension of state S\n",
    "        reward_dim: Dimension of reward R (typically 1)\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        samples_per_task: Number of samples per task per epoch\n",
    "        lr: Learning rate\n",
    "        estimator: MINE estimator type ('dv', 'fdiv', 'nwj')\n",
    "        device: Device to use\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with models, losses, and mi_estimates\n",
    "    \"\"\"\n",
    "    # Create conditioning variable dimensions\n",
    "    X_dim = state_dim + reward_dim + state_dim  # S + R + S'\n",
    "    AX_dim = action_dim + state_dim + reward_dim + state_dim  # A + S + R + S'\n",
    "    \n",
    "    # Initialize MINE model 1: I(Z; A, S, R, S')\n",
    "    mine_args1 = {\n",
    "        'estimator': estimator,\n",
    "        'est_lr': lr,\n",
    "        'variant': 'unbiased',\n",
    "        'device': device\n",
    "    }\n",
    "    mine_model1 = get_estimator(AX_dim, latent_dim, mine_args1)\n",
    "    mine_model1 = mine_model1.to(device)\n",
    "    mine_model1.train()\n",
    "    optimizer1, _ = mine_model1._configure_optimizers()\n",
    "    \n",
    "    # Initialize MINE model 2: I(Z; S, R, S')\n",
    "    mine_args2 = {\n",
    "        'estimator': estimator,\n",
    "        'est_lr': lr,\n",
    "        'variant': 'unbiased',\n",
    "        'device': device\n",
    "    }\n",
    "    mine_model2 = get_estimator(X_dim, latent_dim, mine_args2)\n",
    "    mine_model2 = mine_model2.to(device)\n",
    "    mine_model2.train()\n",
    "    optimizer2, _ = mine_model2._configure_optimizers()\n",
    "    \n",
    "    # Training loop\n",
    "    losses1 = []\n",
    "    losses2 = []\n",
    "    mi_estimates1 = []  # I(Z; A, S, R, S')\n",
    "    mi_estimates2 = []  # I(Z; S, R, S')\n",
    "    mi_conditional = []  # I(Z; A | S, R, S')\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training MINE\"):\n",
    "        epoch_losses1 = []\n",
    "        epoch_losses2 = []\n",
    "        epoch_mi1 = []\n",
    "        epoch_mi2 = []\n",
    "        \n",
    "        # Collect samples from all training tasks\n",
    "        all_data = []\n",
    "        for task_idx in train_tasks:\n",
    "            # Get batch from agent's buffer\n",
    "            data = get_batch_with_latent(agent, task_idx, samples_per_task, is_training=True)\n",
    "            all_data.append(data)\n",
    "        \n",
    "        # Concatenate all tasks\n",
    "        states = torch.cat([d['states'] for d in all_data], dim=0).to(device)\n",
    "        actions = torch.cat([d['actions'] for d in all_data], dim=0).to(device)\n",
    "        rewards = torch.cat([d['rewards'] for d in all_data], dim=0).to(device)\n",
    "        next_states = torch.cat([d['next_states'] for d in all_data], dim=0).to(device)\n",
    "        latents = torch.cat([d['latents'] for d in all_data], dim=0).to(device)\n",
    "        \n",
    "        # Create conditioning variable X = [S, R, S']\n",
    "        X = torch.cat([states, rewards, next_states], dim=1)\n",
    "        \n",
    "        # Create joint variable AX = [A, S, R, S']\n",
    "        AX = torch.cat([actions, states, rewards, next_states], dim=1)\n",
    "        \n",
    "        # Sample batches\n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = n_samples // batch_size\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            batch_idx = indices[i * batch_size:(i + 1) * batch_size]\n",
    "            \n",
    "            # Model 1: I(Z; A, S, R, S')\n",
    "            ax_batch = AX[batch_idx]\n",
    "            z_batch = latents[batch_idx]\n",
    "            shuffle_idx = np.random.permutation(len(z_batch))\n",
    "            z_marginal1 = z_batch[shuffle_idx]\n",
    "            \n",
    "            mi1 = mine_model1.get_mi_bound(ax_batch, z_batch, z_marginal=z_marginal1, \n",
    "                                          update_ema=(epoch > 10))\n",
    "            loss1 = -mi1\n",
    "            \n",
    "            optimizer1.zero_grad()\n",
    "            loss1.backward()\n",
    "            optimizer1.step()\n",
    "            \n",
    "            epoch_losses1.append(loss1.item())\n",
    "            epoch_mi1.append(mi1.item())\n",
    "            \n",
    "            # Model 2: I(Z; S, R, S')\n",
    "            x_batch = X[batch_idx]\n",
    "            z_marginal2 = z_batch[shuffle_idx]\n",
    "            \n",
    "            mi2 = mine_model2.get_mi_bound(x_batch, z_batch, z_marginal=z_marginal2, \n",
    "                                          update_ema=(epoch > 10))\n",
    "            loss2 = -mi2\n",
    "            \n",
    "            optimizer2.zero_grad()\n",
    "            loss2.backward()\n",
    "            optimizer2.step()\n",
    "            \n",
    "            epoch_losses2.append(loss2.item())\n",
    "            epoch_mi2.append(mi2.item())\n",
    "        \n",
    "        mine_model1.step_epoch()\n",
    "        mine_model2.step_epoch()\n",
    "        \n",
    "        avg_loss1 = np.mean(epoch_losses1)\n",
    "        avg_loss2 = np.mean(epoch_losses2)\n",
    "        avg_mi1 = np.mean(epoch_mi1)\n",
    "        avg_mi2 = np.mean(epoch_mi2)\n",
    "        avg_mi_cond = avg_mi1 - avg_mi2  # I(Z; A | S, R, S')\n",
    "        \n",
    "        losses1.append(avg_loss1)\n",
    "        losses2.append(avg_loss2)\n",
    "        mi_estimates1.append(avg_mi1)\n",
    "        mi_estimates2.append(avg_mi2)\n",
    "        mi_conditional.append(avg_mi_cond)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "            print(f\"  I(Z; A, S, R, S') = {avg_mi1:.4f}, I(Z; S, R, S') = {avg_mi2:.4f}\")\n",
    "            print(f\"  I(Z; A | S, R, S') = {avg_mi_cond:.4f}\")\n",
    "    \n",
    "    # Return both models and combined results\n",
    "    results = {\n",
    "        'model_joint': mine_model1,  # I(Z; A, S, R, S')\n",
    "        'model_cond': mine_model2,   # I(Z; S, R, S')\n",
    "        'losses_joint': losses1,\n",
    "        'losses_cond': losses2,\n",
    "        'mi_joint': mi_estimates1,\n",
    "        'mi_cond_base': mi_estimates2,\n",
    "        'mi_conditional': mi_conditional\n",
    "    }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f730ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mine_model(mine_models, agent, eval_tasks, state_dim, reward_dim, \n",
    "                       samples_per_task=1000, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate MINE models on eval tasks to get I(Z; A | S, R, S') estimates\n",
    "    \n",
    "    Args:\n",
    "        mine_models: Dictionary with 'model_joint' and 'model_cond' MINE models\n",
    "        agent: The loaded agent\n",
    "        eval_tasks: List of eval task indices\n",
    "        state_dim: Dimension of state\n",
    "        reward_dim: Dimension of reward\n",
    "        samples_per_task: Number of samples per task for evaluation\n",
    "        device: Device to use\n",
    "    \n",
    "    Returns:\n",
    "        List of MI estimates per task\n",
    "    \"\"\"\n",
    "    mine_model_joint = mine_models['model_joint']\n",
    "    mine_model_cond = mine_models['model_cond']\n",
    "    mine_model_joint.eval()\n",
    "    mine_model_cond.eval()\n",
    "    \n",
    "    mi_estimates = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for task_idx in tqdm(eval_tasks, desc=\"Evaluating on eval tasks\"):\n",
    "            # Get batch from agent's eval buffer\n",
    "            data = get_batch_with_latent(agent, task_idx, samples_per_task, is_training=False)\n",
    "            \n",
    "            states = data['states'].to(device)\n",
    "            actions = data['actions'].to(device)\n",
    "            rewards = data['rewards'].to(device)\n",
    "            next_states = data['next_states'].to(device)\n",
    "            latents = data['latents'].to(device)\n",
    "            \n",
    "            # Create conditioning variable X = [S, R, S']\n",
    "            X = torch.cat([states, rewards, next_states], dim=1)\n",
    "            \n",
    "            # Create joint variable AX = [A, S, R, S']\n",
    "            AX = torch.cat([actions, states, rewards, next_states], dim=1)\n",
    "            \n",
    "            # Evaluate both MI estimates\n",
    "            # Use multiple samples for stable estimate\n",
    "            mi_joint_vals = []\n",
    "            mi_cond_vals = []\n",
    "            n_eval_samples = min(1000, len(X))\n",
    "            \n",
    "            for _ in range(10):  # Multiple evaluations for stability\n",
    "                indices = np.random.choice(len(X), n_eval_samples, replace=False)\n",
    "                \n",
    "                # Evaluate I(Z; A, S, R, S')\n",
    "                ax_batch = AX[indices]\n",
    "                z_batch = latents[indices]\n",
    "                shuffle_idx = np.random.permutation(len(z_batch))\n",
    "                z_marginal1 = z_batch[shuffle_idx]\n",
    "                \n",
    "                mi1 = mine_model_joint.get_mi_bound(ax_batch, z_batch, z_marginal=z_marginal1, update_ema=False)\n",
    "                mi_joint_vals.append(mi1.item())\n",
    "                \n",
    "                # Evaluate I(Z; S, R, S')\n",
    "                x_batch = X[indices]\n",
    "                z_marginal2 = z_batch[shuffle_idx]\n",
    "                \n",
    "                mi2 = mine_model_cond.get_mi_bound(x_batch, z_batch, z_marginal=z_marginal2, update_ema=False)\n",
    "                mi_cond_vals.append(mi2.item())\n",
    "            \n",
    "            # Compute conditional MI: I(Z; A | S, R, S') = I(Z; A, S, R, S') - I(Z; S, R, S')\n",
    "            avg_mi_joint = np.mean(mi_joint_vals)\n",
    "            avg_mi_cond_base = np.mean(mi_cond_vals)\n",
    "            avg_mi_conditional = avg_mi_joint - avg_mi_cond_base\n",
    "            \n",
    "            mi_estimates.append({\n",
    "                'task_idx': task_idx,\n",
    "                'mi_joint': avg_mi_joint,\n",
    "                'mi_cond_base': avg_mi_cond_base,\n",
    "                'mi_estimate': avg_mi_conditional,  # I(Z; A | S, R, S')\n",
    "                'mi_std': np.std([m1 - m2 for m1, m2 in zip(mi_joint_vals, mi_cond_vals)])\n",
    "            })\n",
    "    \n",
    "    return mi_estimates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "12bc6d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train MINE models on training tasks data\n",
    "# print(\"\\nTraining MINE models for I(Z; A | S, R, S')...\")\n",
    "# train_results = train_mine_model(\n",
    "#     train_buffer,\n",
    "#     latent_dim=latent_dim,\n",
    "#     action_dim=action_dim,\n",
    "#     state_dim=state_dim,\n",
    "#     reward_dim=reward_dim,\n",
    "#     epochs=100,\n",
    "#     batch_size=256,\n",
    "#     lr=2e-4,\n",
    "#     estimator='dv',\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "# # Extract results\n",
    "# mine_models = {\n",
    "#     'model_joint': train_results['model_joint'],\n",
    "#     'model_cond': train_results['model_cond']\n",
    "# }\n",
    "# train_losses_joint = train_results['losses_joint']\n",
    "# train_losses_cond = train_results['losses_cond']\n",
    "# train_mi_joint = train_results['mi_joint']\n",
    "# train_mi_cond_base = train_results['mi_cond_base']\n",
    "# train_mi_conditional = train_results['mi_conditional']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86beded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot training progress\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# # Loss plots\n",
    "# axes[0, 0].plot(train_losses_joint, label='I(Z; A, S, R, S\\')')\n",
    "# axes[0, 0].plot(train_losses_cond, label='I(Z; S, R, S\\')')\n",
    "# axes[0, 0].set_xlabel('Epoch')\n",
    "# axes[0, 0].set_ylabel('Loss (Negative MI)')\n",
    "# axes[0, 0].set_title('Training Loss')\n",
    "# axes[0, 0].legend()\n",
    "# axes[0, 0].grid(True)\n",
    "\n",
    "# # MI estimates\n",
    "# axes[0, 1].plot(train_mi_joint, label='I(Z; A, S, R, S\\')')\n",
    "# axes[0, 1].plot(train_mi_cond_base, label='I(Z; S, R, S\\')')\n",
    "# axes[0, 1].set_xlabel('Epoch')\n",
    "# axes[0, 1].set_ylabel('MI Estimate')\n",
    "# axes[0, 1].set_title('MI Estimates during Training')\n",
    "# axes[0, 1].legend()\n",
    "# axes[0, 1].grid(True)\n",
    "\n",
    "# # Conditional MI\n",
    "# axes[1, 0].plot(train_mi_conditional, color='green', linewidth=2)\n",
    "# axes[1, 0].set_xlabel('Epoch')\n",
    "# axes[1, 0].set_ylabel('I(Z; A | S, R, S\\')')\n",
    "# axes[1, 0].set_title('Conditional MI: I(Z; A | S, R, S\\')')\n",
    "# axes[1, 0].grid(True)\n",
    "\n",
    "# # Combined view\n",
    "# axes[1, 1].plot(train_mi_joint, label='I(Z; A, S, R, S\\')', alpha=0.7)\n",
    "# axes[1, 1].plot(train_mi_cond_base, label='I(Z; S, R, S\\')', alpha=0.7)\n",
    "# axes[1, 1].plot(train_mi_conditional, label='I(Z; A | S, R, S\\')', linewidth=2, color='green')\n",
    "# axes[1, 1].set_xlabel('Epoch')\n",
    "# axes[1, 1].set_ylabel('MI Estimate')\n",
    "# axes[1, 1].set_title('All MI Estimates')\n",
    "# axes[1, 1].legend()\n",
    "# axes[1, 1].grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"\\nFinal MI estimates on training data:\")\n",
    "# print(f\"  I(Z; A, S, R, S') = {train_mi_joint[-1]:.4f}\")\n",
    "# print(f\"  I(Z; S, R, S') = {train_mi_cond_base[-1]:.4f}\")\n",
    "# print(f\"  I(Z; A | S, R, S') = {train_mi_conditional[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41751f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate on eval tasks\n",
    "# print(\"\\nEvaluating MINE models on eval tasks...\")\n",
    "# eval_results = evaluate_mine_model(mine_models, eval_buffer, state_dim, reward_dim, device=device)\n",
    "\n",
    "# # Extract MI estimates and task indices\n",
    "# task_indices = [r['task_idx'] for r in eval_results]\n",
    "# mi_joint_vals = [r['mi_joint'] for r in eval_results]\n",
    "# mi_cond_base_vals = [r['mi_cond_base'] for r in eval_results]\n",
    "# mi_values = [r['mi_estimate'] for r in eval_results]  # Conditional MI\n",
    "# mi_stds = [r['mi_std'] for r in eval_results]\n",
    "\n",
    "# print(f\"\\nMI Estimates for eval tasks:\")\n",
    "# for i, (task_idx, mi_j, mi_cb, mi, std) in enumerate(zip(task_indices, mi_joint_vals, mi_cond_base_vals, mi_values, mi_stds)):\n",
    "#     print(f\"  Task {task_idx}: I(Z; A, S, R, S') = {mi_j:.4f}, I(Z; S, R, S') = {mi_cb:.4f}\")\n",
    "#     print(f\"    => I(Z; A | S, R, S') = {mi:.4f} Â± {std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4004041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot results\n",
    "# # Sort by task index for better visualization\n",
    "# sort_idx = np.argsort(task_indices)\n",
    "# sorted_task_indices = np.array(task_indices)[sort_idx]\n",
    "# sorted_mi_joint = np.array(mi_joint_vals)[sort_idx]\n",
    "# sorted_mi_cond_base = np.array(mi_cond_base_vals)[sort_idx]\n",
    "# sorted_mi_values = np.array(mi_values)[sort_idx]\n",
    "# sorted_mi_stds = np.array(mi_stds)[sort_idx]\n",
    "\n",
    "# # Plot 1: Conditional MI with error bars\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "# ax.errorbar(range(len(sorted_task_indices)), sorted_mi_values, yerr=sorted_mi_stds, \n",
    "#             fmt='o-', capsize=5, capthick=2, linewidth=2, markersize=8, color='green', label='I(Z; A | S, R, S\\')')\n",
    "# ax.set_xlabel('Eval Task Index', fontsize=12)\n",
    "# ax.set_ylabel('I(Z; A | S, R, S\\')', fontsize=12)\n",
    "# ax.set_title(f'Conditional Mutual Information I(Z; A | S, R, S\\') for Eval Tasks - {env_name}', fontsize=14)\n",
    "# ax.set_xticks(range(len(sorted_task_indices)))\n",
    "# ax.set_xticklabels(sorted_task_indices, rotation=45, ha='right')\n",
    "# ax.grid(True, alpha=0.3)\n",
    "# ax.axhline(y=train_mi_conditional[-1], color='r', linestyle='--', linewidth=2, \n",
    "#            label=f'Training: {train_mi_conditional[-1]:.4f}')\n",
    "# ax.legend(fontsize=11)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot 2: Bar plot for conditional MI\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "# bars = ax.bar(range(len(sorted_task_indices)), sorted_mi_values, yerr=sorted_mi_stds,\n",
    "#               capsize=5, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "# ax.set_xlabel('Eval Task Index', fontsize=12)\n",
    "# ax.set_ylabel('I(Z; A | S, R, S\\')', fontsize=12)\n",
    "# ax.set_title(f'Conditional Mutual Information I(Z; A | S, R, S\\') for Eval Tasks - {env_name}', fontsize=14)\n",
    "# ax.set_xticks(range(len(sorted_task_indices)))\n",
    "# ax.set_xticklabels(sorted_task_indices, rotation=45, ha='right')\n",
    "# ax.grid(True, alpha=0.3, axis='y')\n",
    "# ax.axhline(y=train_mi_conditional[-1], color='r', linestyle='--', linewidth=2, \n",
    "#            label=f'Training Average: {train_mi_conditional[-1]:.4f}')\n",
    "# ax.legend(fontsize=11)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot 3: Comparison of joint, conditional base, and conditional MI\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "# x_pos = range(len(sorted_task_indices))\n",
    "# width = 0.25\n",
    "\n",
    "# ax.bar([x - width for x in x_pos], sorted_mi_joint, width, label='I(Z; A, S, R, S\\')', alpha=0.7, color='blue')\n",
    "# ax.bar(x_pos, sorted_mi_cond_base, width, label='I(Z; S, R, S\\')', alpha=0.7, color='orange')\n",
    "# ax.bar([x + width for x in x_pos], sorted_mi_values, width, label='I(Z; A | S, R, S\\')', alpha=0.7, color='green', yerr=sorted_mi_stds, capsize=3)\n",
    "\n",
    "# ax.set_xlabel('Eval Task Index', fontsize=12)\n",
    "# ax.set_ylabel('MI Estimate', fontsize=12)\n",
    "# ax.set_title(f'MI Estimates Comparison for Eval Tasks - {env_name}', fontsize=14)\n",
    "# ax.set_xticks(x_pos)\n",
    "# ax.set_xticklabels(sorted_task_indices, rotation=45, ha='right')\n",
    "# ax.legend(fontsize=11)\n",
    "# ax.grid(True, alpha=0.3, axis='y')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc5738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9245d44c",
   "metadata": {},
   "source": [
    "### MINE - Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e38c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66942350",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mine(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Mine, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        nn.init.normal_(self.fc1.weight, std=0.02)\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.normal_(self.fc2.weight, std=0.02)\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "        nn.init.normal_(self.fc3.weight, std=0.02)\n",
    "        nn.init.constant_(self.fc3.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        output = self.fc3(x)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "def mutual_info(joint, marginal, mine_net):\n",
    "    t = mine_net(joint)\n",
    "    et = torch.exp(mine_net(marginal))\n",
    "    mi = torch.mean(t) - torch.log(torch.mean(et))\n",
    "    return mi, t, et\n",
    "\n",
    "\n",
    "def train_mine(info, mine_net, optimizer, ma_et, ma_rate=0.01):\n",
    "    joint, marginal = info\n",
    "    \n",
    "    # Handle both tensor and numpy array inputs\n",
    "    if isinstance(joint, torch.Tensor):\n",
    "        # Already a tensor, just ensure it's on the right device\n",
    "        if not joint.is_cuda:\n",
    "            joint = joint.cuda()\n",
    "    else:\n",
    "        # numpy array or list, convert to tensor\n",
    "        joint = torch.autograd.Variable(torch.FloatTensor(joint)).cuda()\n",
    "    \n",
    "    if isinstance(marginal, torch.Tensor):\n",
    "        # Already a tensor, just ensure it's on the right device\n",
    "        if not marginal.is_cuda:\n",
    "            marginal = marginal.cuda()\n",
    "    else:\n",
    "        # numpy array or list, convert to tensor\n",
    "        marginal = torch.autograd.Variable(torch.FloatTensor(marginal)).cuda()\n",
    "    \n",
    "    # joint = joint.view(joint.size(0), -1)\n",
    "    # marginal = marginal.view(marginal.size(0), -1)\n",
    "    \n",
    "    mi, t, et = mutual_info(joint, marginal, mine_net)\n",
    "    ma_et = (1-ma_rate)*ma_et + ma_rate*torch.mean(et)\n",
    "    \n",
    "    loss = -(torch.mean(t) - (1/ma_et.mean()).detach()*torch.mean(et))\n",
    "    optimizer.zero_grad()\n",
    "    autograd.backward(loss)\n",
    "    # loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), mi, ma_et\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1ba5b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, task_idx, mine_net, optimizer, include_action=True, iter_num = int(2e+4), batch_size = 1024, log_freq = int(1e+2)):\n",
    "    result = list()\n",
    "    ma_et = 1.0\n",
    "    # Auto-determine which buffer to use based on task_idx\n",
    "    # is_training=None will auto-detect from agent.train_tasks and agent.eval_tasks\n",
    "    for i in range(iter_num):\n",
    "        batch = get_batch_with_latent(agent, task_idx, include_action, batch_size, is_training=False)\n",
    "        loss, mi, ma_et = train_mine(batch, mine_net, optimizer, ma_et)\n",
    "        # mi is already a scalar (detached), just append the value\n",
    "        if isinstance(mi, torch.Tensor):\n",
    "            result.append(mi.detach().cpu().item())\n",
    "        else:\n",
    "            result.append(float(mi))\n",
    "        if i % log_freq == 0:\n",
    "            print(f\"Iteration {i}, loss : {loss:.4f}, MI: {mi.item() if isinstance(mi, torch.Tensor) else mi:.4f}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fb238b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma(a, window_size=100):\n",
    "    return [np.mean(a[i:i+window_size]) for i in range(len(a)-window_size+1)]\n",
    "\n",
    "\n",
    "mine_net_1 = Mine(input_dim=state_dim+action_dim+reward_dim+state_dim, hidden_dim=200, output_dim=1).cuda()\n",
    "mine_net_2 = Mine(input_dim=state_dim+reward_dim+state_dim, hidden_dim=200, output_dim=1).cuda()\n",
    "\n",
    "mine_opt_1 = optim.Adam(mine_net_1.parameters(), lr=1e-3)\n",
    "mine_opt_2 = optim.Adam(mine_net_2.parameters(), lr=1e-3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d369ceec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 8, 9, 30, 31, 32, 33, 34]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_tasks = agent.eval_tasks\n",
    "eval_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c3e05ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss : 1.0003, MI: -0.0003\n",
      "Iteration 100, loss : 1.0034, MI: -0.0042\n",
      "Iteration 200, loss : 0.9995, MI: 0.0002\n",
      "Iteration 300, loss : 1.0002, MI: -0.0007\n",
      "Iteration 400, loss : 1.0002, MI: -0.0006\n",
      "Iteration 500, loss : 0.9907, MI: 0.0080\n",
      "Iteration 600, loss : 1.0073, MI: -0.0072\n",
      "Iteration 700, loss : 1.0015, MI: -0.0016\n",
      "Iteration 800, loss : 1.0000, MI: 0.0006\n",
      "Iteration 900, loss : 1.0003, MI: 0.0002\n",
      "Iteration 1000, loss : 1.0002, MI: -0.0005\n",
      "Iteration 1100, loss : 1.0029, MI: -0.0023\n",
      "Iteration 1200, loss : 1.0022, MI: -0.0026\n",
      "Iteration 1300, loss : 1.0038, MI: -0.0034\n",
      "Iteration 1400, loss : 0.9986, MI: 0.0016\n",
      "Iteration 1500, loss : 0.9949, MI: 0.0057\n",
      "Iteration 1600, loss : 1.0109, MI: -0.0082\n",
      "Iteration 1700, loss : 0.9833, MI: 0.0138\n",
      "Iteration 1800, loss : 1.0292, MI: -0.0329\n",
      "Iteration 1900, loss : 0.9975, MI: 0.0105\n",
      "Iteration 2000, loss : 0.8293, MI: 0.1680\n",
      "Iteration 2100, loss : 0.8490, MI: 0.2093\n",
      "Iteration 2200, loss : 0.6987, MI: 0.3130\n",
      "Iteration 2300, loss : 0.4613, MI: 0.5425\n",
      "Iteration 2400, loss : 0.4962, MI: 0.5021\n",
      "Iteration 2500, loss : 0.4069, MI: 0.5977\n",
      "Iteration 2600, loss : 0.3477, MI: 0.7000\n",
      "Iteration 2700, loss : 0.1125, MI: 0.9347\n",
      "Iteration 2800, loss : 0.2741, MI: 0.8298\n",
      "Iteration 2900, loss : -0.1227, MI: 1.1957\n",
      "Iteration 3000, loss : -0.2125, MI: 1.3900\n",
      "Iteration 3100, loss : 0.1653, MI: 0.9657\n",
      "Iteration 3200, loss : -0.1669, MI: 1.3052\n",
      "Iteration 3300, loss : -0.3411, MI: 1.5381\n",
      "Iteration 3400, loss : -0.0177, MI: 1.1819\n",
      "Iteration 3500, loss : -0.5315, MI: 1.7210\n",
      "Iteration 3600, loss : -0.1512, MI: 1.3437\n",
      "Iteration 3700, loss : -0.5297, MI: 1.8179\n",
      "Iteration 3800, loss : -0.6286, MI: 2.0101\n",
      "Iteration 3900, loss : -0.7530, MI: 2.0201\n",
      "Iteration 4000, loss : -0.5648, MI: 1.8151\n",
      "Iteration 4100, loss : -0.1955, MI: 1.4642\n",
      "Iteration 4200, loss : -0.3457, MI: 1.6543\n",
      "Iteration 4300, loss : -0.1731, MI: 1.4951\n",
      "Iteration 4400, loss : -0.2617, MI: 1.6611\n",
      "Iteration 4500, loss : -0.6244, MI: 1.9385\n",
      "Iteration 4600, loss : -0.1117, MI: 1.5278\n",
      "Iteration 4700, loss : -0.3779, MI: 1.7241\n",
      "Iteration 4800, loss : -0.2938, MI: 1.6493\n",
      "Iteration 4900, loss : -0.2020, MI: 1.5676\n",
      "Iteration 5000, loss : -0.5686, MI: 1.9286\n",
      "Iteration 5100, loss : -0.5739, MI: 2.0925\n",
      "Iteration 5200, loss : -0.7309, MI: 2.1067\n",
      "Iteration 5300, loss : -0.8287, MI: 2.2038\n",
      "Iteration 5400, loss : -0.2630, MI: 1.7083\n",
      "Iteration 5500, loss : 0.1938, MI: 1.2512\n",
      "Iteration 5600, loss : -0.7523, MI: 2.1668\n",
      "Iteration 5700, loss : -0.0839, MI: 1.4909\n",
      "Iteration 5800, loss : -1.0531, MI: 2.5778\n",
      "Iteration 5900, loss : -0.8526, MI: 2.2834\n",
      "Iteration 6000, loss : 0.0418, MI: 1.4212\n",
      "Iteration 6100, loss : -0.4031, MI: 1.8827\n",
      "Iteration 6200, loss : -0.5968, MI: 2.1556\n",
      "Iteration 6300, loss : -0.2188, MI: 1.7056\n",
      "Iteration 6400, loss : -0.6869, MI: 2.1794\n",
      "Iteration 6500, loss : 0.3245, MI: 1.1723\n",
      "Iteration 6600, loss : -0.8676, MI: 2.4222\n",
      "Iteration 6700, loss : -0.3020, MI: 1.7868\n",
      "Iteration 6800, loss : -1.0557, MI: 2.6429\n",
      "Iteration 6900, loss : -0.8997, MI: 2.6692\n",
      "Iteration 7000, loss : -0.8266, MI: 2.3382\n",
      "Iteration 7100, loss : -0.6542, MI: 2.1777\n",
      "Iteration 7200, loss : -0.1753, MI: 1.7274\n",
      "Iteration 7300, loss : -0.6917, MI: 2.2364\n",
      "Iteration 7400, loss : -0.6669, MI: 2.2644\n",
      "Iteration 7500, loss : -0.5420, MI: 2.1449\n",
      "Iteration 7600, loss : -0.0482, MI: 1.6995\n",
      "Iteration 7700, loss : -0.6401, MI: 2.1764\n",
      "Iteration 7800, loss : 0.3263, MI: 1.4722\n",
      "Iteration 7900, loss : -0.9009, MI: 2.6811\n",
      "Iteration 8000, loss : -0.2379, MI: 1.8012\n",
      "Iteration 8100, loss : -0.2031, MI: 1.7892\n",
      "Iteration 8200, loss : -0.1179, MI: 1.7504\n",
      "Iteration 8300, loss : -0.4752, MI: 2.0693\n",
      "Iteration 8400, loss : 0.3420, MI: 1.2431\n",
      "Iteration 8500, loss : -0.5303, MI: 2.1156\n",
      "Iteration 8600, loss : -0.7354, MI: 2.3492\n",
      "Iteration 8700, loss : -0.7511, MI: 2.3696\n",
      "Iteration 8800, loss : -0.2359, MI: 1.8709\n",
      "Iteration 8900, loss : -1.0558, MI: 2.6738\n",
      "Iteration 9000, loss : -0.0056, MI: 1.6277\n",
      "Iteration 9100, loss : 0.3056, MI: 1.3623\n",
      "Iteration 9200, loss : -0.3807, MI: 2.0037\n",
      "Iteration 9300, loss : -0.6780, MI: 2.4492\n",
      "Iteration 9400, loss : -0.7927, MI: 2.3767\n",
      "Iteration 9500, loss : -0.8556, MI: 2.7197\n",
      "Iteration 9600, loss : -0.1832, MI: 1.7901\n",
      "Iteration 9700, loss : -1.0475, MI: 2.7637\n",
      "Iteration 9800, loss : -0.2816, MI: 1.8904\n",
      "Iteration 9900, loss : -1.0254, MI: 2.9417\n",
      "Iteration 10000, loss : -0.5869, MI: 2.2172\n",
      "Iteration 10100, loss : -0.7562, MI: 2.4784\n",
      "Iteration 10200, loss : -0.2280, MI: 1.8872\n",
      "Iteration 10300, loss : -0.0693, MI: 1.6807\n",
      "Iteration 10400, loss : -0.8878, MI: 2.6876\n",
      "Iteration 10500, loss : -0.2457, MI: 1.9125\n",
      "Iteration 10600, loss : -0.8824, MI: 2.5256\n",
      "Iteration 10700, loss : 0.2778, MI: 1.3516\n",
      "Iteration 10800, loss : -0.3812, MI: 2.1210\n",
      "Iteration 10900, loss : -0.5619, MI: 2.2156\n",
      "Iteration 11000, loss : -0.2831, MI: 1.9451\n",
      "Iteration 11100, loss : 0.0579, MI: 1.6625\n",
      "Iteration 11200, loss : 0.8148, MI: 0.8772\n",
      "Iteration 11300, loss : -0.8378, MI: 2.5133\n",
      "Iteration 11400, loss : -0.5465, MI: 2.2516\n",
      "Iteration 11500, loss : -0.2523, MI: 1.9010\n",
      "Iteration 11600, loss : -0.5000, MI: 2.1612\n",
      "Iteration 11700, loss : -1.0421, MI: 3.2881\n",
      "Iteration 11800, loss : -0.0485, MI: 1.8235\n",
      "Iteration 11900, loss : -0.1272, MI: 1.8428\n",
      "Iteration 12000, loss : -0.5239, MI: 2.4754\n",
      "Iteration 12100, loss : 0.3111, MI: 1.5623\n",
      "Iteration 12200, loss : 0.0380, MI: 1.7619\n",
      "Iteration 12300, loss : -0.5826, MI: 2.3298\n",
      "Iteration 12400, loss : -0.2246, MI: 1.9336\n",
      "Iteration 12500, loss : -0.4323, MI: 2.1571\n",
      "Iteration 12600, loss : -1.1491, MI: 3.1532\n",
      "Iteration 12700, loss : -1.0424, MI: 2.8401\n",
      "Iteration 12800, loss : -0.1350, MI: 1.9812\n",
      "Iteration 12900, loss : -0.6285, MI: 2.4106\n",
      "Iteration 13000, loss : -0.2792, MI: 2.0596\n",
      "Iteration 13100, loss : -0.6118, MI: 2.3985\n",
      "Iteration 13200, loss : -0.0877, MI: 1.9242\n",
      "Iteration 13300, loss : -0.1158, MI: 1.8577\n",
      "Iteration 13400, loss : -0.8941, MI: 2.8525\n",
      "Iteration 13500, loss : -0.3481, MI: 2.1002\n",
      "Iteration 13600, loss : 0.0227, MI: 1.7201\n",
      "Iteration 13700, loss : -0.4383, MI: 2.2124\n",
      "Iteration 13800, loss : -0.2830, MI: 2.0542\n",
      "Iteration 13900, loss : -0.4770, MI: 2.2610\n",
      "Iteration 14000, loss : -0.4170, MI: 2.1910\n",
      "Iteration 14100, loss : -0.4404, MI: 2.2151\n",
      "Iteration 14200, loss : 0.3566, MI: 1.6903\n",
      "Iteration 14300, loss : -0.1723, MI: 1.9497\n",
      "Iteration 14400, loss : -1.4249, MI: 3.6263\n",
      "Iteration 14500, loss : 0.0518, MI: 1.8032\n",
      "Iteration 14600, loss : 0.3030, MI: 1.6443\n",
      "Iteration 14700, loss : -0.2753, MI: 2.0634\n",
      "Iteration 14800, loss : -0.7987, MI: 2.8325\n",
      "Iteration 14900, loss : 0.1565, MI: 1.6821\n",
      "Iteration 15000, loss : 2.1330, MI: -0.0922\n",
      "Iteration 15100, loss : -0.9453, MI: 2.8057\n",
      "Iteration 15200, loss : -0.4626, MI: 2.3534\n",
      "Iteration 15300, loss : -0.2428, MI: 2.0322\n",
      "Iteration 15400, loss : -0.2540, MI: 2.1183\n",
      "Iteration 15500, loss : -0.2477, MI: 2.0396\n",
      "Iteration 15600, loss : -0.4374, MI: 2.2489\n",
      "Iteration 15700, loss : -0.8925, MI: 2.9146\n",
      "Iteration 15800, loss : -0.1781, MI: 1.9872\n",
      "Iteration 15900, loss : -0.6681, MI: 3.5849\n",
      "Iteration 16000, loss : -0.7650, MI: 3.1996\n",
      "Iteration 16100, loss : -0.6026, MI: 2.5268\n",
      "Iteration 16200, loss : -0.9712, MI: 2.9034\n",
      "Iteration 16300, loss : 0.1251, MI: 1.7961\n",
      "Iteration 16400, loss : -1.0103, MI: 3.0291\n",
      "Iteration 16500, loss : -0.6812, MI: 2.5498\n",
      "Iteration 16600, loss : -1.0044, MI: 3.3173\n",
      "Iteration 16700, loss : -0.4235, MI: 2.2800\n",
      "Iteration 16800, loss : 0.4680, MI: 1.4801\n",
      "Iteration 16900, loss : 0.5506, MI: 1.6840\n",
      "Iteration 17000, loss : -0.0816, MI: 1.9531\n",
      "Iteration 17100, loss : -0.3491, MI: 2.2171\n",
      "Iteration 17200, loss : 0.4271, MI: 1.7572\n",
      "Iteration 17300, loss : -0.9119, MI: 2.9008\n",
      "Iteration 17400, loss : -0.8269, MI: 2.7660\n",
      "Iteration 17500, loss : -0.4498, MI: 2.3286\n",
      "Iteration 17600, loss : 0.0459, MI: 1.8183\n",
      "Iteration 17700, loss : -1.2261, MI: 3.7139\n",
      "Iteration 17800, loss : -0.8301, MI: 2.8226\n",
      "Iteration 17900, loss : 0.6737, MI: 1.4814\n",
      "Iteration 18000, loss : 0.0444, MI: 2.0173\n",
      "Iteration 18100, loss : -0.5335, MI: 2.4383\n",
      "Iteration 18200, loss : -0.0142, MI: 1.8964\n",
      "Iteration 18300, loss : 0.1063, MI: 1.8216\n",
      "Iteration 18400, loss : 0.4216, MI: 1.4490\n",
      "Iteration 18500, loss : -0.2100, MI: 2.1516\n",
      "Iteration 18600, loss : -0.2543, MI: 2.2177\n",
      "Iteration 18700, loss : 0.2926, MI: 1.6254\n",
      "Iteration 18800, loss : -0.2603, MI: 2.1548\n",
      "Iteration 18900, loss : -0.7306, MI: 3.0483\n",
      "Iteration 19000, loss : -0.7228, MI: 2.6526\n",
      "Iteration 19100, loss : -0.7308, MI: 2.6672\n",
      "Iteration 19200, loss : -0.4016, MI: 2.2930\n",
      "Iteration 19300, loss : -0.9195, MI: 3.0534\n",
      "Iteration 19400, loss : -0.0422, MI: 2.0404\n",
      "Iteration 19500, loss : -0.0258, MI: 1.9445\n",
      "Iteration 19600, loss : -0.5188, MI: 2.5949\n",
      "Iteration 19700, loss : 1.3888, MI: 0.5355\n",
      "Iteration 19800, loss : 0.6887, MI: 1.7107\n",
      "Iteration 19900, loss : -0.0882, MI: 1.9984\n",
      "Iteration 0, loss : 1.0002, MI: -0.0002\n",
      "Iteration 100, loss : 1.0002, MI: -0.0004\n",
      "Iteration 200, loss : 0.9831, MI: 0.0177\n",
      "Iteration 300, loss : 1.0009, MI: -0.0003\n",
      "Iteration 400, loss : 1.0012, MI: -0.0001\n",
      "Iteration 500, loss : 1.0082, MI: -0.0072\n",
      "Iteration 600, loss : 1.0021, MI: -0.0012\n",
      "Iteration 700, loss : 1.0090, MI: -0.0063\n",
      "Iteration 800, loss : 1.0026, MI: -0.0007\n",
      "Iteration 900, loss : 1.0010, MI: -0.0001\n",
      "Iteration 1000, loss : 1.0082, MI: -0.0081\n",
      "Iteration 1100, loss : 1.0026, MI: -0.0011\n",
      "Iteration 1200, loss : 1.0027, MI: 0.0002\n",
      "Iteration 1300, loss : 1.0056, MI: -0.0061\n",
      "Iteration 1400, loss : 0.9910, MI: 0.0104\n",
      "Iteration 1500, loss : 1.0058, MI: -0.0000\n",
      "Iteration 1600, loss : 0.8552, MI: 0.1473\n",
      "Iteration 1700, loss : 0.8837, MI: 0.1301\n",
      "Iteration 1800, loss : 0.7124, MI: 0.2886\n",
      "Iteration 1900, loss : 0.6070, MI: 0.4408\n",
      "Iteration 2000, loss : 0.6537, MI: 0.4000\n",
      "Iteration 2100, loss : 0.3776, MI: 0.6395\n",
      "Iteration 2200, loss : 0.2603, MI: 0.7844\n",
      "Iteration 2300, loss : 0.0331, MI: 1.0194\n",
      "Iteration 2400, loss : 0.1410, MI: 0.9595\n",
      "Iteration 2500, loss : 0.0724, MI: 1.0534\n",
      "Iteration 2600, loss : 0.1939, MI: 0.9158\n",
      "Iteration 2700, loss : -0.3638, MI: 1.4812\n",
      "Iteration 2800, loss : -0.3284, MI: 1.5010\n",
      "Iteration 2900, loss : 0.0376, MI: 1.1511\n",
      "Iteration 3000, loss : -0.4144, MI: 1.5764\n",
      "Iteration 3100, loss : -0.7106, MI: 2.0040\n",
      "Iteration 3200, loss : -0.1438, MI: 1.3386\n",
      "Iteration 3300, loss : -0.2717, MI: 1.4810\n",
      "Iteration 3400, loss : -0.4282, MI: 1.6359\n",
      "Iteration 3500, loss : -0.3239, MI: 1.5504\n",
      "Iteration 3600, loss : 0.3725, MI: 0.9338\n",
      "Iteration 3700, loss : 0.0023, MI: 1.3015\n",
      "Iteration 3800, loss : -0.4449, MI: 1.6780\n",
      "Iteration 3900, loss : -0.4617, MI: 1.7092\n",
      "Iteration 4000, loss : -0.7626, MI: 2.0394\n",
      "Iteration 4100, loss : -0.9875, MI: 2.2630\n",
      "Iteration 4200, loss : -0.5163, MI: 1.8198\n",
      "Iteration 4300, loss : -1.0056, MI: 2.3906\n",
      "Iteration 4400, loss : -1.0398, MI: 2.3758\n",
      "Iteration 4500, loss : -0.4405, MI: 1.7198\n",
      "Iteration 4600, loss : -0.7891, MI: 2.1122\n",
      "Iteration 4700, loss : -0.7391, MI: 2.0441\n",
      "Iteration 4800, loss : -0.5255, MI: 1.8226\n",
      "Iteration 4900, loss : -0.6185, MI: 1.9233\n",
      "Iteration 5000, loss : -0.2217, MI: 1.6963\n",
      "Iteration 5100, loss : -0.8034, MI: 2.1357\n",
      "Iteration 5200, loss : -1.1117, MI: 2.5300\n",
      "Iteration 5300, loss : 0.2216, MI: 1.2352\n",
      "Iteration 5400, loss : -0.4759, MI: 1.8828\n",
      "Iteration 5500, loss : -0.6497, MI: 1.9936\n",
      "Iteration 5600, loss : -1.0066, MI: 2.6299\n",
      "Iteration 5700, loss : -0.2762, MI: 1.6544\n",
      "Iteration 5800, loss : 0.2985, MI: 1.1110\n",
      "Iteration 5900, loss : -0.2949, MI: 1.6949\n",
      "Iteration 6000, loss : -0.1783, MI: 1.5596\n",
      "Iteration 6100, loss : -0.6479, MI: 2.0373\n",
      "Iteration 6200, loss : -0.4760, MI: 1.8586\n",
      "Iteration 6300, loss : -0.2098, MI: 1.7766\n",
      "Iteration 6400, loss : -0.3493, MI: 1.7315\n",
      "Iteration 6500, loss : -0.4540, MI: 1.8633\n",
      "Iteration 6600, loss : -0.0603, MI: 1.4643\n",
      "Iteration 6700, loss : -0.6225, MI: 2.0141\n",
      "Iteration 6800, loss : -0.8767, MI: 2.2639\n",
      "Iteration 6900, loss : -0.5545, MI: 1.9668\n",
      "Iteration 7000, loss : -1.2268, MI: 2.8642\n",
      "Iteration 7100, loss : -0.8097, MI: 2.2232\n",
      "Iteration 7200, loss : -0.3186, MI: 1.7825\n",
      "Iteration 7300, loss : -0.4431, MI: 1.8553\n",
      "Iteration 7400, loss : -1.1839, MI: 2.6450\n",
      "Iteration 7500, loss : -0.7466, MI: 2.2246\n",
      "Iteration 7600, loss : -0.2697, MI: 1.8202\n",
      "Iteration 7700, loss : -0.6244, MI: 2.0518\n",
      "Iteration 7800, loss : -0.3551, MI: 1.7795\n",
      "Iteration 7900, loss : -0.8259, MI: 2.2939\n",
      "Iteration 8000, loss : -0.5138, MI: 1.9661\n",
      "Iteration 8100, loss : -0.2459, MI: 1.7439\n",
      "Iteration 8200, loss : -0.9521, MI: 2.4096\n",
      "Iteration 8300, loss : -0.2471, MI: 1.7019\n",
      "Iteration 8400, loss : -0.2693, MI: 1.8626\n",
      "Iteration 8500, loss : -1.0346, MI: 2.6047\n",
      "Iteration 8600, loss : -0.0098, MI: 1.5572\n",
      "Iteration 8700, loss : -0.2627, MI: 1.7267\n",
      "Iteration 8800, loss : -0.3661, MI: 1.8425\n",
      "Iteration 8900, loss : -1.2660, MI: 3.3063\n",
      "Iteration 9000, loss : -0.4836, MI: 2.0707\n",
      "Iteration 9100, loss : -0.2115, MI: 1.7571\n",
      "Iteration 9200, loss : -0.3720, MI: 1.8410\n",
      "Iteration 9300, loss : -0.9914, MI: 2.6770\n",
      "Iteration 9400, loss : 0.1469, MI: 1.6570\n",
      "Iteration 9500, loss : -0.4761, MI: 1.9777\n",
      "Iteration 9600, loss : -0.6586, MI: 2.1542\n",
      "Iteration 9700, loss : -0.7617, MI: 2.2606\n",
      "Iteration 9800, loss : -0.5338, MI: 2.0361\n",
      "Iteration 9900, loss : -0.5130, MI: 2.0737\n",
      "Iteration 10000, loss : -0.5717, MI: 2.1150\n",
      "Iteration 10100, loss : -0.6856, MI: 2.2081\n",
      "Iteration 10200, loss : 0.3448, MI: 1.4349\n",
      "Iteration 10300, loss : -0.2428, MI: 1.8170\n",
      "Iteration 10400, loss : -0.2796, MI: 1.8188\n",
      "Iteration 10500, loss : -0.0677, MI: 1.6567\n",
      "Iteration 10600, loss : -0.2691, MI: 1.8513\n",
      "Iteration 10700, loss : -0.8651, MI: 2.4643\n",
      "Iteration 10800, loss : -0.5886, MI: 2.1227\n",
      "Iteration 10900, loss : -0.7236, MI: 2.3546\n",
      "Iteration 11000, loss : 0.0965, MI: 1.5099\n",
      "Iteration 11100, loss : -0.6817, MI: 2.1973\n",
      "Iteration 11200, loss : -0.3147, MI: 1.8463\n",
      "Iteration 11300, loss : -0.7448, MI: 2.2713\n",
      "Iteration 11400, loss : -0.5534, MI: 2.1129\n",
      "Iteration 11500, loss : -0.5408, MI: 2.0555\n",
      "Iteration 11600, loss : -0.5951, MI: 2.1184\n",
      "Iteration 11700, loss : -0.9581, MI: 2.7473\n",
      "Iteration 11800, loss : -1.3885, MI: 3.1515\n",
      "Iteration 11900, loss : -0.9036, MI: 2.5381\n",
      "Iteration 12000, loss : -0.3652, MI: 1.9401\n",
      "Iteration 12100, loss : -0.8838, MI: 2.4564\n",
      "Iteration 12200, loss : -0.2625, MI: 1.8419\n",
      "Iteration 12300, loss : -0.3365, MI: 1.9009\n",
      "Iteration 12400, loss : -0.8845, MI: 2.4924\n",
      "Iteration 12500, loss : 0.0506, MI: 1.7214\n",
      "Iteration 12600, loss : -0.1596, MI: 1.7343\n",
      "Iteration 12700, loss : -0.9619, MI: 2.6658\n",
      "Iteration 12800, loss : -0.6784, MI: 2.2517\n",
      "Iteration 12900, loss : -0.6916, MI: 2.5250\n",
      "Iteration 13000, loss : -0.7244, MI: 2.3144\n",
      "Iteration 13100, loss : -0.6341, MI: 2.2328\n",
      "Iteration 13200, loss : -0.6910, MI: 2.3804\n",
      "Iteration 13300, loss : -0.8383, MI: 2.6049\n",
      "Iteration 13400, loss : -0.8768, MI: 2.6142\n",
      "Iteration 13500, loss : -0.9087, MI: 2.5384\n",
      "Iteration 13600, loss : -0.9180, MI: 2.5884\n",
      "Iteration 13700, loss : -0.1164, MI: 1.8101\n",
      "Iteration 13800, loss : -0.9873, MI: 2.6148\n",
      "Iteration 13900, loss : -0.1228, MI: 1.7962\n",
      "Iteration 14000, loss : -0.8128, MI: 2.4182\n",
      "Iteration 14100, loss : -0.3496, MI: 1.9601\n",
      "Iteration 14200, loss : -1.0618, MI: 3.0205\n",
      "Iteration 14300, loss : -0.1293, MI: 1.8982\n",
      "Iteration 14400, loss : -0.0582, MI: 1.7222\n",
      "Iteration 14500, loss : -0.7946, MI: 2.5677\n",
      "Iteration 14600, loss : 0.0341, MI: 1.6967\n",
      "Iteration 14700, loss : -0.5581, MI: 2.1937\n",
      "Iteration 14800, loss : -0.9131, MI: 2.5496\n",
      "Iteration 14900, loss : -0.7826, MI: 2.4191\n",
      "Iteration 15000, loss : -0.3248, MI: 1.9901\n",
      "Iteration 15100, loss : -0.9046, MI: 2.6451\n",
      "Iteration 15200, loss : -0.6684, MI: 2.2985\n",
      "Iteration 15300, loss : -0.4586, MI: 2.1013\n",
      "Iteration 15400, loss : -1.1241, MI: 2.8083\n",
      "Iteration 15500, loss : -0.4296, MI: 2.0636\n",
      "Iteration 15600, loss : 0.4585, MI: 1.4731\n",
      "Iteration 15700, loss : -0.5769, MI: 2.1886\n",
      "Iteration 15800, loss : -0.1008, MI: 1.7786\n",
      "Iteration 15900, loss : -0.4277, MI: 2.0992\n",
      "Iteration 16000, loss : -0.3208, MI: 1.9697\n",
      "Iteration 16100, loss : -0.3423, MI: 2.0425\n",
      "Iteration 16200, loss : -1.0854, MI: 2.7983\n",
      "Iteration 16300, loss : -0.4761, MI: 2.1073\n",
      "Iteration 16400, loss : -0.7609, MI: 2.4055\n",
      "Iteration 16500, loss : -0.0982, MI: 1.8236\n",
      "Iteration 16600, loss : -1.2846, MI: 3.3903\n",
      "Iteration 16700, loss : -0.8628, MI: 3.2826\n",
      "Iteration 16800, loss : -0.7457, MI: 2.4548\n",
      "Iteration 16900, loss : -1.1840, MI: 3.0515\n",
      "Iteration 17000, loss : -0.4981, MI: 2.1589\n",
      "Iteration 17100, loss : -0.2837, MI: 1.9966\n",
      "Iteration 17200, loss : -0.1593, MI: 1.7762\n",
      "Iteration 17300, loss : -0.7448, MI: 2.3820\n",
      "Iteration 17400, loss : -0.6893, MI: 2.3106\n",
      "Iteration 17500, loss : -0.2170, MI: 1.8768\n",
      "Iteration 17600, loss : -0.2215, MI: 1.8714\n",
      "Iteration 17700, loss : -0.7923, MI: 2.7946\n",
      "Iteration 17800, loss : 0.2486, MI: 1.5661\n",
      "Iteration 17900, loss : 0.0700, MI: 1.7758\n",
      "Iteration 18000, loss : -0.2011, MI: 1.9337\n",
      "Iteration 18100, loss : -0.7139, MI: 2.4025\n",
      "Iteration 18200, loss : -0.7423, MI: 2.3875\n",
      "Iteration 18300, loss : -0.9435, MI: 2.6171\n",
      "Iteration 18400, loss : -1.0529, MI: 2.9450\n",
      "Iteration 18500, loss : -0.0466, MI: 1.7417\n",
      "Iteration 18600, loss : -0.8040, MI: 2.5338\n",
      "Iteration 18700, loss : -1.1175, MI: 3.1208\n",
      "Iteration 18800, loss : -1.0414, MI: 2.7670\n",
      "Iteration 18900, loss : 0.3770, MI: 1.5960\n",
      "Iteration 19000, loss : -0.3859, MI: 2.1063\n",
      "Iteration 19100, loss : 0.0328, MI: 1.6459\n",
      "Iteration 19200, loss : -0.5733, MI: 2.2667\n",
      "Iteration 19300, loss : -0.4981, MI: 2.1715\n",
      "Iteration 19400, loss : -0.3366, MI: 2.0050\n",
      "Iteration 19500, loss : -0.4958, MI: 2.2421\n",
      "Iteration 19600, loss : -0.1614, MI: 1.8788\n",
      "Iteration 19700, loss : -1.2229, MI: 3.2211\n",
      "Iteration 19800, loss : -0.9776, MI: 2.7523\n",
      "Iteration 19900, loss : -0.3708, MI: 2.0971\n",
      "0.09719924449920647\n"
     ]
    }
   ],
   "source": [
    "task_idx = 5\n",
    "mine_net_1 = Mine(input_dim=state_dim+action_dim+reward_dim+state_dim, hidden_dim=200, output_dim=1).cuda()\n",
    "mine_net_2 = Mine(input_dim=state_dim+reward_dim+state_dim, hidden_dim=200, output_dim=1).cuda()\n",
    "mine_opt_1 = optim.Adam(mine_net_1.parameters(), lr=1e-3)\n",
    "mine_opt_2 = optim.Adam(mine_net_2.parameters(), lr=1e-3)\n",
    "result_1 = train(agent, task_idx, mine_net_1, mine_opt_1, include_action=True, iter_num=int(2e+4), batch_size=64, log_freq=int(1e+2))\n",
    "result_2 = train(agent, task_idx, mine_net_2, mine_opt_2, include_action=False, iter_num=int(2e+4), batch_size=64, log_freq=int(1e+2))\n",
    "\n",
    "ma_result_1 = ma(result_1)\n",
    "ma_result_2 = ma(result_2)\n",
    "\n",
    "conditional_MI = ma_result_1[-1] - ma_result_2[-1]\n",
    "print(conditional_MI)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79656b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
